{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_format(value):\n",
    "    return \"{:,}\".format(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./pa1-data/\"\n",
    "files_name = dict()\n",
    "\n",
    "for i in range(10):\n",
    "    dir_path = base_path + str(i)\n",
    "    files = os.listdir(dir_path)\n",
    "    files = [file for file in files if file != '.DS_Store']\n",
    "    files_name[i] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9', '0', '7', '6', '1', '8', '4', '3', '2', '5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = os.listdir(base_path)\n",
    "[file for file in l if file != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = []\n",
    "with open(\"CACM/common_words\", \"r\") as cw:\n",
    "    common_words = [ l.replace(\"\\n\", \"\") for l in cw.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_document(base_path, dir_path, file_name):\n",
    "    with open(base_path + dir_path + file_name, \"r\") as f:\n",
    "        current_document_tokens = []\n",
    "\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line.replace(\"\\n\", \"\")\n",
    "            lower_line = line.lower()\n",
    "            list_split = re.split(\"\\W+\", lower_line)\n",
    "            for t in list_split:\n",
    "                if t not in common_words and t != \"\":\n",
    "                    current_document_tokens += [t]\n",
    "\n",
    "        return current_document_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./pa1-data/\"\n",
    "\n",
    "docid_names = {}\n",
    "tokens = {}\n",
    "\n",
    "doc_id = 1\n",
    "for directory in files_name.keys():\n",
    "    dir_path = str(directory) + '/'\n",
    "    for file_name in files_name[directory]:\n",
    "        docid_names[doc_id] = dir_path + file_name\n",
    "        tokens[doc_id] = get_tokens_document(base_path, dir_path, file_name)\n",
    "        doc_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document: 98,999\n"
     ]
    }
   ],
   "source": [
    "nb_documents = doc_id\n",
    "print(\"Number of document: \" + nice_format(nb_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = set()\n",
    "\n",
    "for key in tokens.keys():\n",
    "    vocabulaire.update(tokens[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille vocabulaire: 325,984\n"
     ]
    }
   ],
   "source": [
    "taille_vocabulaire = len(vocabulaire)\n",
    "print(\"Taille vocabulaire: \" + nice_format(taille_vocabulaire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens: 17,719,205\n"
     ]
    }
   ],
   "source": [
    "nb_tokens = sum([len(tokens[x]) for x in tokens.keys()])\n",
    "print(\"Nombre de tokens: \" + nice_format(nb_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_moitie = {}\n",
    "for i in range(1, int(nb_documents/2) + 1):\n",
    "    tokens_moitie[i] = tokens[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens pour la moitie des documents: 9,841,885\n"
     ]
    }
   ],
   "source": [
    "nb_tokens_moitie = sum([len(tokens_moitie[x]) for x in tokens_moitie])\n",
    "print(\"Nombre de tokens pour la moitie des documents: \" + nice_format(nb_tokens_moitie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire_moitie = set()\n",
    "\n",
    "for key in tokens_moitie.keys():\n",
    "    vocabulaire_moitie.update(tokens_moitie[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens pour la moitie des documents: 184,925\n"
     ]
    }
   ],
   "source": [
    "taille_vocabulaire_moitie = len(vocabulaire_moitie)\n",
    "print(\"Nombre de tokens pour la moitie des documents: \" + nice_format(taille_vocabulaire_moitie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b=0.9641091946757916\n"
     ]
    }
   ],
   "source": [
    "b = log(float(taille_vocabulaire)/taille_vocabulaire_moitie)/log(float(nb_tokens)/nb_tokens_moitie)\n",
    "print (\"b=\" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.03348918300041858\n"
     ]
    }
   ],
   "source": [
    "k = taille_vocabulaire/pow(nb_tokens,b)\n",
    "print (\"k=\" +str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,396.69119331342\n"
     ]
    }
   ],
   "source": [
    "print (nice_format(k * pow(1e6,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences = {}\n",
    "\n",
    "for key in tokens.keys():\n",
    "    for token in tokens[key]:\n",
    "        if token not in frequences.keys():\n",
    "            frequences[token] = 1\n",
    "        else:\n",
    "            frequences[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(frequences.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_value.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stanford', 363488),\n",
       " ('university', 134427),\n",
       " ('1', 128831),\n",
       " ('research', 105426),\n",
       " ('home', 86017),\n",
       " ('center', 83196),\n",
       " ('2', 80492),\n",
       " ('page', 80104),\n",
       " ('contact', 78372),\n",
       " ('search', 70452)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_value[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [i+1 for i in range(len(sorted_by_value))]\n",
    "ys = [y[1] for y in sorted_by_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_xs = [log(i+1) for i in range(len(sorted_by_value))]\n",
    "ln_ys = [log(y[1]) for y in sorted_by_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(ln_xs, ln_ys, 'bo', label='Frequence')\n",
    "# plt.legend()\n",
    "# plt.title('ln(frequence)=f(ln(rang))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ids = {}\n",
    "for (i,k) in enumerate(frequences):\n",
    "    term_ids[k] = i\n",
    "\n",
    "term_ids_inverse = {v: k for k, v in term_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequence_token(token, document_id):\n",
    "    return tokens[document_id].count(token) / len(tokens[document_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "taille_bloc = 100\n",
    "splitted_tokens = {}\n",
    "for (i,k) in enumerate(tokens):\n",
    "    if i//taille_bloc in splitted_tokens:\n",
    "        splitted_tokens[i//taille_bloc][k] = tokens[k]\n",
    "    else:\n",
    "        splitted_tokens[i//taille_bloc] = {}\n",
    "        splitted_tokens[i//taille_bloc][k] = tokens[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inverse = {}\n",
    "for bloc in splitted_tokens.values():\n",
    "    buffer = []\n",
    "    for (i,k) in enumerate(bloc):\n",
    "        for pair in bloc[k]:\n",
    "            buffer += [(term_ids[pair], k)]\n",
    "    sorted_index = sorted(buffer, key=lambda kv: kv[0])\n",
    "    for pair in sorted_index:\n",
    "        term_id, doc_id = pair\n",
    "        if term_id in index_inverse:\n",
    "            index_inverse[term_id] += [(doc_id, frequence_token(term_ids_inverse[term_id], doc_id))]\n",
    "        else:\n",
    "            index_inverse[term_id] = [(doc_id, frequence_token(term_ids_inverse[term_id], doc_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_request_and(token1, token2):\n",
    "    id_1 = term_ids[token1]\n",
    "    id_2 = term_ids[token2]\n",
    "        \n",
    "    list1 = [pair[0] for pair in index_inverse[id_1]] if id_1 in index_inverse else []\n",
    "    list2 = [pair[0] for pair in index_inverse[id_2]] if id_2 in index_inverse else []\n",
    "        \n",
    "    return set(list1).intersection(set(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_request_or(token1, token2):\n",
    "    id_1 = term_ids[token1]\n",
    "    id_2 = term_ids[token2]\n",
    "    \n",
    "    list1 = [pair[0] for pair in index_inverse[id_1]] if id_1 in index_inverse else []\n",
    "    list2 = [pair[0] for pair in index_inverse[id_2]] if id_2 in index_inverse else []\n",
    "        \n",
    "    return set(list1).union(set(list2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = set(tokens.keys())\n",
    "def boolean_request_not(token1):\n",
    "    id_1 = term_ids[token1]\n",
    "    \n",
    "    list1 = [pair[0] for pair in index_inverse[id_1]] if id_1 in index_inverse else []\n",
    "        \n",
    "    return document_ids.difference(set(list1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_request_not_ensemble(set1):\n",
    "    return document_ids.difference(set1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requete_arbre(arbre):\n",
    "    if len(arbre) == 1:\n",
    "        id_1 = term_ids[arbre[0]]\n",
    "        list1 = [pair[0] for pair in index_inverse[id_1]] if id_1 in index_inverse else []\n",
    "        \n",
    "        return set(list1)\n",
    "    \n",
    "    operator = arbre[1]\n",
    "    if operator == 'AND':\n",
    "        return requete_arbre(arbre[0]).intersection(requete_arbre(arbre[2]))\n",
    "    elif operator == 'OR':\n",
    "        return requete_arbre(arbre[0]).union(requete_arbre(arbre[2]))\n",
    "    elif operator == 'NOT':\n",
    "        return boolean_request_not_ensemble(requete_arbre(arbre[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "taille_documents = {}\n",
    "for (i,k) in enumerate(tokens):\n",
    "    taille_documents[k] = sqrt(sum([pow(frequence_token(tok, k),2) for tok in tokens[k]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = {}\n",
    "for k in index_inverse:\n",
    "    idfs[k] = log(nb_documents/len(index_inverse[k]),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3424182939871017"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinus_tf(requete):\n",
    "    similarites = {}\n",
    "    norme_requete = sqrt(sum([pow(requete.count(terme)/len(requete),2) for terme in requete]))\n",
    "    for terme in requete:\n",
    "        terme_id = term_ids[terme]\n",
    "        freq_terme_in_requete = requete.count(terme)/len(requete)\n",
    "        for (documentId, freq) in index_inverse[terme_id]:\n",
    "            if documentId in similarites:\n",
    "                similarites[documentId] += freq*freq_terme_in_requete/(norme_requete*taille_documents[documentId])\n",
    "            else:\n",
    "                similarites[documentId] = freq*freq_terme_in_requete/(norme_requete*taille_documents[documentId])\n",
    "    \n",
    "    return sorted(similarites.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11769, 3.2934186664251794),\n",
       " (604, 3.2392726799655573),\n",
       " (2611, 3.154316277589963),\n",
       " (53212, 2.374536067453556),\n",
       " (52718, 2.0561587888491846),\n",
       " (27159, 1.9521208050960102),\n",
       " (72311, 1.916054224539098),\n",
       " (71569, 1.7023689888471423),\n",
       " (17494, 1.5141278278751513),\n",
       " (16384, 1.4288732936180237)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinus_tf(['assignment', 'restrictions'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinus_tf_idf_log(requete):\n",
    "    similarites = {}\n",
    "    norme_requete = sqrt(sum([pow(requete.count(terme)/len(requete),2) for terme in requete]))\n",
    "    for terme in requete:\n",
    "        terme_id = term_ids[terme]\n",
    "        poids_terme_in_requete = requete.count(terme)/(len(requete)*idfs[terme_id])\n",
    "        for (documentId, freq) in index_inverse[terme_id]:\n",
    "            poids = 1 + log(freq/idfs[terme_id])\n",
    "            if documentId in similarites:\n",
    "                similarites[documentId] += poids*poids_terme_in_requete/(norme_requete*taille_documents[documentId])\n",
    "            else:\n",
    "                similarites[documentId] = poids*poids_terme_in_requete/(norme_requete*taille_documents[documentId])\n",
    "    return sorted(similarites.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37457, -1.0728672793497671),\n",
       " (68227, -1.2400616681687582),\n",
       " (86719, -1.3033332676845009),\n",
       " (75882, -1.3148055803698344),\n",
       " (92732, -1.4338585574040932),\n",
       " (33682, -1.5946774102422079),\n",
       " (65588, -1.6563233290648487),\n",
       " (83735, -1.8066779930830594),\n",
       " (68835, -1.8099225548681708),\n",
       " (44860, -1.8295746057360716)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinus_tf_idf_log(['assignment', 'restrictions'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
